{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e32b23e",
   "metadata": {},
   "source": [
    "# Nh·∫≠n di·ªán khu√¥n m·∫∑t th·ªùi gian th·ª±c v·ªõi FaceNet & MTCNN tr√™n Webcam\n",
    "\n",
    "B√†i t·∫≠p th·ª±c h√†nh: X√¢y d·ª±ng h·ªá th·ªëng nh·∫≠n di·ªán khu√¥n m·∫∑t theo th·ªùi gian th·ª±c s·ª≠ d·ª•ng:\n",
    "- **MTCNN**: Ph√°t hi·ªán khu√¥n m·∫∑t\n",
    "- **FaceNet**: Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng v√† so s√°nh khu√¥n m·∫∑t\n",
    "- **OpenCV**: Truy c·∫≠p webcam\n",
    "\n",
    "**ƒêi·ªÅu ki·ªán so s√°nh:**\n",
    "- Similarity > 0.7: \"Matched\"\n",
    "- Similarity < 0.7: \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f8265fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ import c√°c th∆∞ vi·ªán v√† thi·∫øt l·∫≠p config\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "from keras_facenet import FaceNet\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "KNOWN_DIR = \"known\"          # th∆∞ m·ª•c ch·ª©a ·∫£nh ng∆∞·ªùi ƒë√£ bi·∫øt\n",
    "THRESHOLD = 0.7             # similarity threshold\n",
    "CAM_INDEX = 0               # webcam index (0 th∆∞·ªùng l√† webcam m·∫∑c ƒë·ªãnh)\n",
    "MIN_FACE_SIZE = 60          # b·ªè qua m·∫∑t qu√° nh·ªè (px)\n",
    "\n",
    "print(\"‚úÖ ƒê√£ import c√°c th∆∞ vi·ªán v√† thi·∫øt l·∫≠p config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a7d04a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ ƒë·ªãnh nghƒ©a c√°c helper functions\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "def l2_normalize(x: np.ndarray, eps: float = 1e-10) -> np.ndarray:\n",
    "    \"\"\"Chu·∫©n h√≥a vector theo L2\"\"\"\n",
    "    return x / (np.linalg.norm(x) + eps)\n",
    "\n",
    "def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    \"\"\"T√≠nh ƒë·ªô t∆∞∆°ng ƒë·ªìng cosine gi·ªØa 2 vectors\"\"\"\n",
    "    a = l2_normalize(a)\n",
    "    b = l2_normalize(b)\n",
    "    return float(np.dot(a, b))\n",
    "\n",
    "def preprocess_face(face_bgr: np.ndarray, target_size=(160, 160)) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    FaceNet th∆∞·ªùng d√πng input 160x160, RGB.\n",
    "    Tr·∫£ v·ªÅ m·∫£ng (160,160,3) RGB uint8.\n",
    "    \"\"\"\n",
    "    face_rgb = cv2.cvtColor(face_bgr, cv2.COLOR_BGR2RGB)\n",
    "    face_rgb = cv2.resize(face_rgb, target_size, interpolation=cv2.INTER_AREA)\n",
    "    return face_rgb\n",
    "\n",
    "def get_embedding(embedder: FaceNet, face_rgb_160: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    keras-facenet: embeddings() nh·∫≠n list/np array ·∫£nh RGB (uint8 ok).\n",
    "    \"\"\"\n",
    "    emb = embedder.embeddings([face_rgb_160])[0]  # shape (512,)\n",
    "    return emb.astype(np.float32)\n",
    "\n",
    "def safe_crop(frame: np.ndarray, x: int, y: int, w: int, h: int) -> tuple:\n",
    "    \"\"\"Crop an to√†n, ƒë·∫£m b·∫£o kh√¥ng v∆∞·ª£t bi√™n\"\"\"\n",
    "    H, W = frame.shape[:2]\n",
    "    x1 = max(0, x)\n",
    "    y1 = max(0, y)\n",
    "    x2 = min(W, x + w)\n",
    "    y2 = min(H, y + h)\n",
    "    return frame[y1:y2, x1:x2], (x1, y1, x2, y2)\n",
    "\n",
    "print(\"‚úÖ ƒê√£ ƒë·ªãnh nghƒ©a c√°c helper functions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e28436c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang load models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <_io.BufferedReader>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\lz4\\frame\\__init__.py\", line 753, in flush\n",
      "    self._fp.flush()\n",
      "ValueError: I/O operation on closed file.\n",
      "Exception ignored in: <_io.BufferedReader>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\lz4\\frame\\__init__.py\", line 753, in flush\n",
      "    self._fp.flush()\n",
      "ValueError: I/O operation on closed file.\n",
      "Exception ignored in: <_io.BufferedReader>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\lz4\\frame\\__init__.py\", line 753, in flush\n",
      "    self._fp.flush()\n",
      "ValueError: I/O operation on closed file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ MTCNN v√† FaceNet ƒë√£ s·∫µn s√†ng!\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# LOAD MODELS\n",
    "# =========================\n",
    "print(\"ƒêang load models...\")\n",
    "detector = MTCNN()\n",
    "embedder = FaceNet()  # load FaceNet\n",
    "print(\"‚úÖ MTCNN v√† FaceNet ƒë√£ s·∫µn s√†ng!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "89bd5189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] ƒêang t·∫°o database embeddings t·ª´ th∆∞ m·ª•c known/ ...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
      "    + Loaded: Cristiano_Ronaldo_2275_(cropped)\n",
      "[!] Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c: known\\ronaldo.png\n",
      "\n",
      "‚úÖ Database s·∫µn s√†ng: 1 ng∆∞·ªùi\n",
      "üìã Danh s√°ch: Cristiano_Ronaldo_2275_(cropped)\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# BUILD KNOWN DATABASE\n",
    "# =========================\n",
    "known_embeddings = []\n",
    "known_names = []\n",
    "\n",
    "# T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥\n",
    "if not os.path.isdir(KNOWN_DIR):\n",
    "    os.makedirs(KNOWN_DIR, exist_ok=True)\n",
    "    print(f\"[!] ƒê√£ t·∫°o th∆∞ m·ª•c '{KNOWN_DIR}'. H√£y b·ªè ·∫£nh ng∆∞·ªùi quen v√†o ƒë√≥ r·ªìi ch·∫°y l·∫°i.\")\n",
    "else:\n",
    "    image_files = [f for f in os.listdir(KNOWN_DIR) if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "    \n",
    "    if len(image_files) == 0:\n",
    "        print(f\"[!] Th∆∞ m·ª•c '{KNOWN_DIR}' ch∆∞a c√≥ ·∫£nh. H√£y th√™m ·∫£nh (jpg/png) r·ªìi ch·∫°y l·∫°i.\")\n",
    "    else:\n",
    "        print(f\"[*] ƒêang t·∫°o database embeddings t·ª´ th∆∞ m·ª•c {KNOWN_DIR}/ ...\")\n",
    "        \n",
    "        for fn in image_files:\n",
    "            path = os.path.join(KNOWN_DIR, fn)\n",
    "            img = cv2.imread(path)\n",
    "            if img is None:\n",
    "                print(f\"[!] Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c: {path}\")\n",
    "                continue\n",
    "\n",
    "            # detect face in known image\n",
    "            faces = detector.detect_faces(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "            if len(faces) == 0:\n",
    "                print(f\"[!] Kh√¥ng ph√°t hi·ªán m·∫∑t trong: {fn}\")\n",
    "                continue\n",
    "\n",
    "            # l·∫•y face c√≥ confidence cao nh·∫•t\n",
    "            faces = sorted(faces, key=lambda d: d.get(\"confidence\", 0), reverse=True)\n",
    "            x, y, w, h = faces[0][\"box\"]\n",
    "            face_crop, _ = safe_crop(img, x, y, w, h)\n",
    "\n",
    "            if face_crop.size == 0 or min(face_crop.shape[:2]) < MIN_FACE_SIZE:\n",
    "                print(f\"[!] M·∫∑t qu√° nh·ªè/l·ªói crop trong: {fn}\")\n",
    "                continue\n",
    "\n",
    "            face_160 = preprocess_face(face_crop)\n",
    "            emb = get_embedding(embedder, face_160)\n",
    "\n",
    "            name = os.path.splitext(fn)[0]  # t√™n = filename kh√¥ng ƒëu√¥i\n",
    "            known_embeddings.append(emb)\n",
    "            known_names.append(name)\n",
    "            print(f\"    + Loaded: {name}\")\n",
    "\n",
    "        known_embeddings = np.array(known_embeddings, dtype=np.float32)\n",
    "        \n",
    "        if len(known_embeddings) == 0:\n",
    "            print(\"[!] Kh√¥ng t·∫°o ƒë∆∞·ª£c embeddings n√†o t·ª´ known/. H√£y d√πng ·∫£nh r√µ m·∫∑t h∆°n.\")\n",
    "        else:\n",
    "            print(f\"\\n‚úÖ Database s·∫µn s√†ng: {len(known_embeddings)} ng∆∞·ªùi\")\n",
    "            print(f\"üìã Danh s√°ch: {', '.join(known_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7af5013d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Webcam ƒë√£ s·∫µn s√†ng. Nh·∫•n 'q' ƒë·ªÉ tho√°t.\n",
      "[*] Threshold: 0.7\n",
      "[*] S·ªë ng∆∞·ªùi trong database: 1\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "\n",
      "‚úÖ ƒê√£ ƒë√≥ng webcam v√† gi·∫£i ph√≥ng t√†i nguy√™n\n",
      "üìä T·ªïng s·ªë frames x·ª≠ l√Ω: 52\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# REALTIME WEBCAM\n",
    "# =========================\n",
    "if len(known_embeddings) == 0:\n",
    "    print(\"‚ö†Ô∏è  Kh√¥ng c√≥ database ƒë·ªÉ so s√°nh. H√£y th√™m ·∫£nh v√†o th∆∞ m·ª•c 'known/' tr∆∞·ªõc!\")\n",
    "else:\n",
    "    cap = cv2.VideoCapture(CAM_INDEX)\n",
    "    if not cap.isOpened():\n",
    "        print(\"[!] Kh√¥ng m·ªü ƒë∆∞·ª£c webcam.\")\n",
    "    else:\n",
    "        print(\"[*] Webcam ƒë√£ s·∫µn s√†ng. Nh·∫•n 'q' ƒë·ªÉ tho√°t.\")\n",
    "        print(f\"[*] Threshold: {THRESHOLD}\")\n",
    "        print(f\"[*] S·ªë ng∆∞·ªùi trong database: {len(known_names)}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        frame_count = 0\n",
    "        \n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Flip frame ƒë·ªÉ t·∫°o hi·ªáu ·ª©ng g∆∞∆°ng\n",
    "            frame = cv2.flip(frame, 1)\n",
    "            \n",
    "            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            detections = detector.detect_faces(rgb)\n",
    "\n",
    "            for det in detections:\n",
    "                conf = det.get(\"confidence\", 0)\n",
    "                if conf < 0.90:  # b·ªè qua detection c√≥ confidence th·∫•p\n",
    "                    continue\n",
    "\n",
    "                x, y, w, h = det[\"box\"]\n",
    "                face_crop, (x1, y1, x2, y2) = safe_crop(frame, x, y, w, h)\n",
    "\n",
    "                if face_crop.size == 0:\n",
    "                    continue\n",
    "                if min(face_crop.shape[:2]) < MIN_FACE_SIZE:\n",
    "                    continue\n",
    "\n",
    "                face_160 = preprocess_face(face_crop)\n",
    "                emb_live = get_embedding(embedder, face_160)\n",
    "\n",
    "                # So kh·ªõp: l·∫•y similarity l·ªõn nh·∫•t\n",
    "                sims = [cosine_similarity(emb_live, e) for e in known_embeddings]\n",
    "                best_idx = int(np.argmax(sims))\n",
    "                best_sim = float(sims[best_idx])\n",
    "                best_name = known_names[best_idx]\n",
    "\n",
    "                # Ki·ªÉm tra threshold\n",
    "                if best_sim > THRESHOLD:\n",
    "                    label = \"Matched\"\n",
    "                    color = (0, 255, 0)  # XANH L√Å\n",
    "                    display_text = f\"{best_name} | sim={best_sim:.2f}\"\n",
    "                else:\n",
    "                    label = \"Unknown\"\n",
    "                    color = (0, 0, 255)  # ƒê·ªé\n",
    "                    display_text = f\"Unknown | sim={best_sim:.2f}\"\n",
    "\n",
    "                # V·∫Ω bounding box\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "                \n",
    "                # V·∫Ω background cho text\n",
    "                (tw, th), _ = cv2.getTextSize(display_text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n",
    "                cv2.rectangle(frame, (x1, y1-30), (x1+tw, y1), color, -1)\n",
    "                \n",
    "                # V·∫Ω text\n",
    "                cv2.putText(frame, display_text, (x1, y1-10), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "                \n",
    "                # V·∫Ω confidence\n",
    "                cv2.putText(frame, f\"Conf: {conf:.2f}\", (x1, y2+20),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "            # Hi·ªÉn th·ªã h∆∞·ªõng d·∫´n\n",
    "            cv2.putText(frame, \"Press 'q' to quit\", (10, 30),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "            \n",
    "            # Hi·ªÉn th·ªã frame\n",
    "            cv2.imshow(\"FaceNet + MTCNN Realtime\", frame)\n",
    "            \n",
    "            frame_count += 1\n",
    "            \n",
    "            if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "                break\n",
    "\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        print(\"\\n‚úÖ ƒê√£ ƒë√≥ng webcam v√† gi·∫£i ph√≥ng t√†i nguy√™n\")\n",
    "        print(f\"üìä T·ªïng s·ªë frames x·ª≠ l√Ω: {frame_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e975f236",
   "metadata": {},
   "source": [
    "## üìù Gi·∫£i th√≠ch Logic Ch√≠nh X√°c\n",
    "\n",
    "### üîç Lu·ªìng x·ª≠ l√Ω:\n",
    "\n",
    "1. **Load Models**\n",
    "   - MTCNN: Detector ph√°t hi·ªán khu√¥n m·∫∑t\n",
    "   - FaceNet: Tr√≠ch xu·∫•t embedding 512 chi·ªÅu\n",
    "\n",
    "2. **Build Database** \n",
    "   - ƒê·ªçc ·∫£nh t·ª´ th∆∞ m·ª•c `known/`\n",
    "   - Detect face b·∫±ng MTCNN\n",
    "   - Tr√≠ch xu·∫•t embedding b·∫±ng FaceNet\n",
    "   - L∆∞u v√†o arrays `known_embeddings` v√† `known_names`\n",
    "\n",
    "3. **Real-time Recognition**\n",
    "   - Capture frame t·ª´ webcam\n",
    "   - Detect faces trong frame (MTCNN)\n",
    "   - V·ªõi m·ªói face:\n",
    "     - Crop v√† resize v·ªÅ 160x160\n",
    "     - Tr√≠ch xu·∫•t embedding\n",
    "     - T√≠nh cosine similarity v·ªõi t·∫•t c·∫£ known embeddings\n",
    "     - L·∫•y similarity cao nh·∫•t\n",
    "     - **ƒêi·ªÅu ki·ªán**: \n",
    "       - `similarity > 0.7` ‚Üí **Matched** (XANH L√Å) + hi·ªÉn th·ªã t√™n\n",
    "       - `similarity ‚â§ 0.7` ‚Üí **Unknown** (ƒê·ªé)\n",
    "\n",
    "### ‚öôÔ∏è C√°c tham s·ªë quan tr·ªçng:\n",
    "\n",
    "- **THRESHOLD = 0.7**: Ng∆∞·ª°ng ph√¢n bi·ªát matched/unknown\n",
    "- **MIN_FACE_SIZE = 60px**: B·ªè qua face qu√° nh·ªè\n",
    "- **Confidence MTCNN > 0.90**: Ch·ªâ x·ª≠ l√Ω detection c√≥ confidence cao"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
