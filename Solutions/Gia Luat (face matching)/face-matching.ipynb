{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e32b23e",
   "metadata": {},
   "source": [
    "# Nh·∫≠n di·ªán khu√¥n m·∫∑t th·ªùi gian th·ª±c v·ªõi FaceNet & MTCNN tr√™n Webcam\n",
    "\n",
    "B√†i t·∫≠p th·ª±c h√†nh: X√¢y d·ª±ng h·ªá th·ªëng nh·∫≠n di·ªán khu√¥n m·∫∑t theo th·ªùi gian th·ª±c s·ª≠ d·ª•ng:\n",
    "- **MTCNN**: Ph√°t hi·ªán khu√¥n m·∫∑t\n",
    "- **FaceNet**: Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng v√† so s√°nh khu√¥n m·∫∑t\n",
    "- **OpenCV**: Truy c·∫≠p webcam\n",
    "\n",
    "**ƒêi·ªÅu ki·ªán so s√°nh:**\n",
    "- Similarity > 0.7: \"Matched\"\n",
    "- Similarity < 0.7: \"Unknown\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82254472",
   "metadata": {},
   "source": [
    "## 1. Import th∆∞ vi·ªán v√† thi·∫øt l·∫≠p c·∫•u h√¨nh\n",
    "\n",
    "Ph·∫ßn n√†y import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt v√† thi·∫øt l·∫≠p c√°c tham s·ªë quan tr·ªçng:\n",
    "- **MTCNN**: Th∆∞ vi·ªán ph√°t hi·ªán khu√¥n m·∫∑t v·ªõi ƒë·ªô ch√≠nh x√°c cao\n",
    "- **FaceNet**: M√¥ h√¨nh deep learning ƒë·ªÉ tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng khu√¥n m·∫∑t (embeddings 512 chi·ªÅu)\n",
    "- **OpenCV**: X·ª≠ l√Ω ·∫£nh v√† video\n",
    "\n",
    "**C√°c tham s·ªë c·∫•u h√¨nh:**\n",
    "- `KNOWN_DIR`: Th∆∞ m·ª•c ch·ª©a ·∫£nh c·ªßa nh·ªØng ng∆∞·ªùi c·∫ßn nh·∫≠n di·ªán\n",
    "- `THRESHOLD`: Ng∆∞·ª°ng similarity ƒë·ªÉ x√°c ƒë·ªãnh \"Matched\" (0.7 = 70% ƒë·ªô t∆∞∆°ng ƒë·ªìng)\n",
    "- `CAM_INDEX`: Ch·ªâ s·ªë webcam (0 = webcam m·∫∑c ƒë·ªãnh)\n",
    "- `MIN_FACE_SIZE`: K√≠ch th∆∞·ªõc t·ªëi thi·ªÉu c·ªßa khu√¥n m·∫∑t ƒë∆∞·ª£c x·ª≠ l√Ω (pixel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8265fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ import c√°c th∆∞ vi·ªán v√† thi·∫øt l·∫≠p config\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "from keras_facenet import FaceNet\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "KNOWN_DIR = \"known\"          # th∆∞ m·ª•c ch·ª©a ·∫£nh ng∆∞·ªùi ƒë√£ bi·∫øt\n",
    "THRESHOLD = 0.7             # similarity threshold\n",
    "CAM_INDEX = 0               # webcam index (0 th∆∞·ªùng l√† webcam m·∫∑c ƒë·ªãnh)\n",
    "MIN_FACE_SIZE = 60          # b·ªè qua m·∫∑t qu√° nh·ªè (px)\n",
    "\n",
    "print(\"‚úÖ ƒê√£ import c√°c th∆∞ vi·ªán v√† thi·∫øt l·∫≠p config\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be90375",
   "metadata": {},
   "source": [
    "## 2. ƒê·ªãnh nghƒ©a c√°c h√†m h·ªó tr·ª£ (Helper Functions)\n",
    "\n",
    "C√°c h√†m ti·ªán √≠ch ƒë·ªÉ x·ª≠ l√Ω embeddings v√† ·∫£nh:\n",
    "\n",
    "- **`l2_normalize()`**: Chu·∫©n h√≥a vector theo chu·∫©n L2 (ƒë∆∞a v·ªÅ ƒë·ªô d√†i = 1)\n",
    "- **`cosine_similarity()`**: T√≠nh ƒë·ªô t∆∞∆°ng ƒë·ªìng cosine gi·ªØa 2 embeddings (k·∫øt qu·∫£ t·ª´ -1 ƒë·∫øn 1, c√†ng g·∫ßn 1 = c√†ng gi·ªëng)\n",
    "- **`preprocess_face()`**: Ti·ªÅn x·ª≠ l√Ω khu√¥n m·∫∑t:\n",
    "  - Chuy·ªÉn t·ª´ BGR ‚Üí RGB (OpenCV d√πng BGR, FaceNet c·∫ßn RGB)\n",
    "  - Resize v·ªÅ 160√ó160 pixels (k√≠ch th∆∞·ªõc input c·ªßa FaceNet)\n",
    "- **`get_embedding()`**: Tr√≠ch xu·∫•t vector ƒë·∫∑c tr∆∞ng 512 chi·ªÅu t·ª´ ·∫£nh khu√¥n m·∫∑t\n",
    "- **`safe_crop()`**: Crop ·∫£nh an to√†n kh√¥ng b·ªã v∆∞·ª£t bi√™n frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7d04a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ ƒë·ªãnh nghƒ©a c√°c helper functions\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "def l2_normalize(x: np.ndarray, eps: float = 1e-10) -> np.ndarray:\n",
    "    \"\"\"Chu·∫©n h√≥a vector theo L2\"\"\"\n",
    "    return x / (np.linalg.norm(x) + eps)\n",
    "\n",
    "def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    \"\"\"T√≠nh ƒë·ªô t∆∞∆°ng ƒë·ªìng cosine gi·ªØa 2 vectors\"\"\"\n",
    "    a = l2_normalize(a)\n",
    "    b = l2_normalize(b)\n",
    "    return float(np.dot(a, b))\n",
    "\n",
    "def preprocess_face(face_bgr: np.ndarray, target_size=(160, 160)) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    FaceNet th∆∞·ªùng d√πng input 160x160, RGB.\n",
    "    Tr·∫£ v·ªÅ m·∫£ng (160,160,3) RGB uint8.\n",
    "    \"\"\"\n",
    "    face_rgb = cv2.cvtColor(face_bgr, cv2.COLOR_BGR2RGB)\n",
    "    face_rgb = cv2.resize(face_rgb, target_size, interpolation=cv2.INTER_AREA)\n",
    "    return face_rgb\n",
    "\n",
    "def get_embedding(embedder: FaceNet, face_rgb_160: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    keras-facenet: embeddings() nh·∫≠n list/np array ·∫£nh RGB (uint8 ok).\n",
    "    \"\"\"\n",
    "    emb = embedder.embeddings([face_rgb_160])[0]  # shape (512,)\n",
    "    return emb.astype(np.float32)\n",
    "\n",
    "def safe_crop(frame: np.ndarray, x: int, y: int, w: int, h: int) -> tuple:\n",
    "    \"\"\"Crop an to√†n, ƒë·∫£m b·∫£o kh√¥ng v∆∞·ª£t bi√™n\"\"\"\n",
    "    H, W = frame.shape[:2]\n",
    "    x1 = max(0, x)\n",
    "    y1 = max(0, y)\n",
    "    x2 = min(W, x + w)\n",
    "    y2 = min(H, y + h)\n",
    "    return frame[y1:y2, x1:x2], (x1, y1, x2, y2)\n",
    "\n",
    "print(\"‚úÖ ƒê√£ ƒë·ªãnh nghƒ©a c√°c helper functions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88b5fee",
   "metadata": {},
   "source": [
    "## 3. Kh·ªüi t·∫°o c√°c m√¥ h√¨nh AI\n",
    "\n",
    "Load 2 m√¥ h√¨nh deep learning ch√≠nh:\n",
    "\n",
    "1. **MTCNN (Multi-task Cascaded Convolutional Networks)**:\n",
    "   - Ph√°t hi·ªán khu√¥n m·∫∑t trong ·∫£nh/video\n",
    "   - Tr·∫£ v·ªÅ bounding box v√† confidence score\n",
    "   - Ho·∫°t ƒë·ªông t·ªët v·ªõi nhi·ªÅu khu√¥n m·∫∑t, g√≥c nghi√™ng, v√† ƒëi·ªÅu ki·ªán √°nh s√°ng kh√°c nhau\n",
    "\n",
    "2. **FaceNet (keras-facenet)**:\n",
    "   - M√¥ h√¨nh CNN pre-trained tr√™n h√†ng tri·ªáu khu√¥n m·∫∑t\n",
    "   - Chuy·ªÉn ƒë·ªïi ·∫£nh khu√¥n m·∫∑t th√†nh vector 512 chi·ªÅu\n",
    "   - C√°c khu√¥n m·∫∑t gi·ªëng nhau c√≥ embeddings g·∫ßn nhau trong kh√¥ng gian vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e28436c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <_io.BufferedReader>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\lz4\\frame\\__init__.py\", line 753, in flush\n",
      "    self._fp.flush()\n",
      "ValueError: I/O operation on closed file.\n",
      "Exception ignored in: <_io.BufferedReader>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\lz4\\frame\\__init__.py\", line 753, in flush\n",
      "    self._fp.flush()\n",
      "ValueError: I/O operation on closed file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang load models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <_io.BufferedReader>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\lz4\\frame\\__init__.py\", line 753, in flush\n",
      "    self._fp.flush()\n",
      "ValueError: I/O operation on closed file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ MTCNN v√† FaceNet ƒë√£ s·∫µn s√†ng!\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# LOAD MODELS\n",
    "# =========================\n",
    "print(\"ƒêang load models...\")\n",
    "detector = MTCNN()\n",
    "embedder = FaceNet()  # load FaceNet\n",
    "print(\"‚úÖ MTCNN v√† FaceNet ƒë√£ s·∫µn s√†ng!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e7eb51",
   "metadata": {},
   "source": [
    "## 4. X√¢y d·ª±ng c∆° s·ªü d·ªØ li·ªáu khu√¥n m·∫∑t (Database)\n",
    "\n",
    "T·∫°o database embeddings t·ª´ ·∫£nh trong th∆∞ m·ª•c `known/`:\n",
    "\n",
    "**Quy tr√¨nh:**\n",
    "1. T·∫°o th∆∞ m·ª•c `known/` n·∫øu ch∆∞a t·ªìn t·∫°i\n",
    "2. Duy·ªát qua t·∫•t c·∫£ file ·∫£nh (.jpg, .jpeg, .png)\n",
    "3. V·ªõi m·ªói ·∫£nh:\n",
    "   - ƒê·ªçc ·∫£nh v√† ph√°t hi·ªán khu√¥n m·∫∑t b·∫±ng MTCNN\n",
    "   - L·∫•y khu√¥n m·∫∑t c√≥ confidence cao nh·∫•t (n·∫øu c√≥ nhi·ªÅu ng∆∞·ªùi)\n",
    "   - Crop v√† preprocess khu√¥n m·∫∑t\n",
    "   - Tr√≠ch xu·∫•t embedding 512 chi·ªÅu b·∫±ng FaceNet\n",
    "   - L∆∞u embedding v√† t√™n ng∆∞·ªùi (t√™n file kh√¥ng ƒëu√¥i) v√†o arrays\n",
    "4. Chuy·ªÉn sang numpy array ƒë·ªÉ t√≠nh to√°n hi·ªáu qu·∫£\n",
    "\n",
    "**K·∫øt qu·∫£:** Danh s√°ch `known_embeddings` v√† `known_names` ƒë·ªÉ so s√°nh real-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89bd5189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] ƒêang t·∫°o database embeddings t·ª´ th∆∞ m·ª•c known/ ...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
      "    + Loaded: Cristiano_Ronaldo_2275_(cropped)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "    + Loaded: gia luat\n",
      "\n",
      "‚úÖ Database s·∫µn s√†ng: 2 ng∆∞·ªùi\n",
      "üìã Danh s√°ch: Cristiano_Ronaldo_2275_(cropped), gia luat\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# BUILD KNOWN DATABASE\n",
    "# =========================\n",
    "known_embeddings = []\n",
    "known_names = []\n",
    "\n",
    "# T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥\n",
    "if not os.path.isdir(KNOWN_DIR):\n",
    "    os.makedirs(KNOWN_DIR, exist_ok=True)\n",
    "    print(f\"[!] ƒê√£ t·∫°o th∆∞ m·ª•c '{KNOWN_DIR}'. H√£y b·ªè ·∫£nh ng∆∞·ªùi quen v√†o ƒë√≥ r·ªìi ch·∫°y l·∫°i.\")\n",
    "else:\n",
    "    image_files = [f for f in os.listdir(KNOWN_DIR) if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "    \n",
    "    if len(image_files) == 0:\n",
    "        print(f\"[!] Th∆∞ m·ª•c '{KNOWN_DIR}' ch∆∞a c√≥ ·∫£nh. H√£y th√™m ·∫£nh (jpg/png) r·ªìi ch·∫°y l·∫°i.\")\n",
    "    else:\n",
    "        print(f\"[*] ƒêang t·∫°o database embeddings t·ª´ th∆∞ m·ª•c {KNOWN_DIR}/ ...\")\n",
    "        \n",
    "        for fn in image_files:\n",
    "            path = os.path.join(KNOWN_DIR, fn)\n",
    "            img = cv2.imread(path)\n",
    "            if img is None:\n",
    "                print(f\"[!] Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c: {path}\")\n",
    "                continue\n",
    "\n",
    "            # detect face in known image\n",
    "            faces = detector.detect_faces(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "            if len(faces) == 0:\n",
    "                print(f\"[!] Kh√¥ng ph√°t hi·ªán m·∫∑t trong: {fn}\")\n",
    "                continue\n",
    "\n",
    "            # l·∫•y face c√≥ confidence cao nh·∫•t\n",
    "            faces = sorted(faces, key=lambda d: d.get(\"confidence\", 0), reverse=True)\n",
    "            x, y, w, h = faces[0][\"box\"]\n",
    "            face_crop, _ = safe_crop(img, x, y, w, h)\n",
    "\n",
    "            if face_crop.size == 0 or min(face_crop.shape[:2]) < MIN_FACE_SIZE:\n",
    "                print(f\"[!] M·∫∑t qu√° nh·ªè/l·ªói crop trong: {fn}\")\n",
    "                continue\n",
    "\n",
    "            face_160 = preprocess_face(face_crop)\n",
    "            emb = get_embedding(embedder, face_160)\n",
    "\n",
    "            name = os.path.splitext(fn)[0]  # t√™n = filename kh√¥ng ƒëu√¥i\n",
    "            known_embeddings.append(emb)\n",
    "            known_names.append(name)\n",
    "            print(f\"    + Loaded: {name}\")\n",
    "\n",
    "        known_embeddings = np.array(known_embeddings, dtype=np.float32)\n",
    "        \n",
    "        if len(known_embeddings) == 0:\n",
    "            print(\"[!] Kh√¥ng t·∫°o ƒë∆∞·ª£c embeddings n√†o t·ª´ known/. H√£y d√πng ·∫£nh r√µ m·∫∑t h∆°n.\")\n",
    "        else:\n",
    "            print(f\"\\n‚úÖ Database s·∫µn s√†ng: {len(known_embeddings)} ng∆∞·ªùi\")\n",
    "            print(f\"üìã Danh s√°ch: {', '.join(known_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dbdd77",
   "metadata": {},
   "source": [
    "## 5. Nh·∫≠n di·ªán khu√¥n m·∫∑t theo th·ªùi gian th·ª±c (Real-time Recognition)\n",
    "\n",
    "X·ª≠ l√Ω video t·ª´ webcam v√† nh·∫≠n di·ªán khu√¥n m·∫∑t frame-by-frame:\n",
    "\n",
    "**Lu·ªìng x·ª≠ l√Ω m·ªói frame:**\n",
    "1. Capture frame t·ª´ webcam v√† flip ngang (hi·ªáu ·ª©ng g∆∞∆°ng)\n",
    "2. Chuy·ªÉn sang RGB v√† ph√°t hi·ªán khu√¥n m·∫∑t b·∫±ng MTCNN\n",
    "3. V·ªõi m·ªói khu√¥n m·∫∑t ƒë∆∞·ª£c ph√°t hi·ªán:\n",
    "   - Ki·ªÉm tra confidence > 0.90 (l·ªçc detection k√©m)\n",
    "   - Crop v√† preprocess khu√¥n m·∫∑t\n",
    "   - Tr√≠ch xu·∫•t embedding\n",
    "   - **So s√°nh v·ªõi database**: T√≠nh cosine similarity v·ªõi t·∫•t c·∫£ known embeddings\n",
    "   - L·∫•y similarity cao nh·∫•t v√† ki·ªÉm tra threshold:\n",
    "     - **> 0.7**: Matched ‚Üí v·∫Ω box XANH L√Å + hi·ªÉn th·ªã t√™n\n",
    "     - **‚â§ 0.7**: Unknown ‚Üí v·∫Ω box ƒê·ªé\n",
    "4. Hi·ªÉn th·ªã frame v·ªõi bounding boxes, t√™n, v√† similarity scores\n",
    "5. Nh·∫•n 'q' ƒë·ªÉ tho√°t\n",
    "\n",
    "**T·ªëi ∆∞u:**\n",
    "- B·ªè qua khu√¥n m·∫∑t qu√° nh·ªè (< 60px)\n",
    "- Hi·ªÉn th·ªã confidence c·ªßa MTCNN\n",
    "- ƒê·∫øm s·ªë frames ƒë√£ x·ª≠ l√Ω"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7af5013d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Webcam ƒë√£ s·∫µn s√†ng. Nh·∫•n 'q' ƒë·ªÉ tho√°t.\n",
      "[*] Threshold: 0.7\n",
      "[*] S·ªë ng∆∞·ªùi trong database: 2\n",
      "--------------------------------------------------\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\n",
      "‚úÖ ƒê√£ ƒë√≥ng webcam v√† gi·∫£i ph√≥ng t√†i nguy√™n\n",
      "üìä T·ªïng s·ªë frames x·ª≠ l√Ω: 23\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# REALTIME WEBCAM\n",
    "# =========================\n",
    "if len(known_embeddings) == 0:\n",
    "    print(\"‚ö†Ô∏è  Kh√¥ng c√≥ database ƒë·ªÉ so s√°nh. H√£y th√™m ·∫£nh v√†o th∆∞ m·ª•c 'known/' tr∆∞·ªõc!\")\n",
    "else:\n",
    "    cap = cv2.VideoCapture(CAM_INDEX)\n",
    "    if not cap.isOpened():\n",
    "        print(\"[!] Kh√¥ng m·ªü ƒë∆∞·ª£c webcam.\")\n",
    "    else:\n",
    "        print(\"[*] Webcam ƒë√£ s·∫µn s√†ng. Nh·∫•n 'q' ƒë·ªÉ tho√°t.\")\n",
    "        print(f\"[*] Threshold: {THRESHOLD}\")\n",
    "        print(f\"[*] S·ªë ng∆∞·ªùi trong database: {len(known_names)}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        frame_count = 0\n",
    "        \n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Flip frame ƒë·ªÉ t·∫°o hi·ªáu ·ª©ng g∆∞∆°ng\n",
    "            frame = cv2.flip(frame, 1)\n",
    "            \n",
    "            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            detections = detector.detect_faces(rgb)\n",
    "\n",
    "            for det in detections:\n",
    "                conf = det.get(\"confidence\", 0)\n",
    "                if conf < 0.90:  # b·ªè qua detection c√≥ confidence th·∫•p\n",
    "                    continue\n",
    "\n",
    "                x, y, w, h = det[\"box\"]\n",
    "                face_crop, (x1, y1, x2, y2) = safe_crop(frame, x, y, w, h)\n",
    "\n",
    "                if face_crop.size == 0:\n",
    "                    continue\n",
    "                if min(face_crop.shape[:2]) < MIN_FACE_SIZE:\n",
    "                    continue\n",
    "\n",
    "                face_160 = preprocess_face(face_crop)\n",
    "                emb_live = get_embedding(embedder, face_160)\n",
    "\n",
    "                # So kh·ªõp: l·∫•y similarity l·ªõn nh·∫•t\n",
    "                sims = [cosine_similarity(emb_live, e) for e in known_embeddings]\n",
    "                best_idx = int(np.argmax(sims))\n",
    "                best_sim = float(sims[best_idx])\n",
    "                best_name = known_names[best_idx]\n",
    "\n",
    "                # Ki·ªÉm tra threshold\n",
    "                if best_sim > THRESHOLD:\n",
    "                    label = \"Matched\"\n",
    "                    color = (0, 255, 0)  # XANH L√Å\n",
    "                    # display_text = f\"{best_name} | sim={best_sim:.2f}\"  #hi·ªÉn th·ªã t√™n\n",
    "                    display_text = f\"MATCHED | sim={best_sim:.2f}\"\n",
    "                else:\n",
    "                    label = \"Unknown\"\n",
    "                    color = (0, 0, 255)  # ƒê·ªé\n",
    "                    display_text = f\"UNKNOWN | sim={best_sim:.2f}\"\n",
    "\n",
    "                # V·∫Ω bounding box\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "                \n",
    "                # V·∫Ω background cho text\n",
    "                (tw, th), _ = cv2.getTextSize(display_text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n",
    "                cv2.rectangle(frame, (x1, y1-30), (x1+tw, y1), color, -1)\n",
    "                \n",
    "                # V·∫Ω text\n",
    "                cv2.putText(frame, display_text, (x1, y1-10), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "                \n",
    "                # V·∫Ω confidence\n",
    "                cv2.putText(frame, f\"Conf: {conf:.2f}\", (x1, y2+20),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "            # Hi·ªÉn th·ªã h∆∞·ªõng d·∫´n\n",
    "            cv2.putText(frame, \"Press 'q' to quit\", (10, 30),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "            \n",
    "            # Hi·ªÉn th·ªã frame\n",
    "            cv2.imshow(\"FaceNet + MTCNN Realtime\", frame)\n",
    "            \n",
    "            frame_count += 1\n",
    "            \n",
    "            if cv2.waitKey(25) & 0xFF == ord(\"q\"):\n",
    "                break\n",
    "\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        print(\"\\n‚úÖ ƒê√£ ƒë√≥ng webcam v√† gi·∫£i ph√≥ng t√†i nguy√™n\")\n",
    "        print(f\"üìä T·ªïng s·ªë frames x·ª≠ l√Ω: {frame_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e975f236",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéØ T·ªïng K·∫øt: √ù T∆∞·ªüng v√† C√°ch Tri·ªÉn Khai\n",
    "\n",
    "## üìå √ù t∆∞·ªüng b√†i to√°n\n",
    "\n",
    "X√¢y d·ª±ng h·ªá th·ªëng **nh·∫≠n di·ªán khu√¥n m·∫∑t theo th·ªùi gian th·ª±c** c√≥ kh·∫£ nƒÉng:\n",
    "- Ph√°t hi·ªán khu√¥n m·∫∑t trong video stream t·ª´ webcam\n",
    "- So s√°nh v·ªõi database ng∆∞·ªùi ƒë√£ bi·∫øt\n",
    "- Ph√¢n lo·∫°i: \"Matched\" (ng∆∞·ªùi quen) ho·∫∑c \"Unknown\" (ng∆∞·ªùi l·∫°)\n",
    "- Hi·ªÉn th·ªã k·∫øt qu·∫£ tr·ª±c quan v·ªõi bounding boxes v√† t√™n ng∆∞·ªùi\n",
    "\n",
    "## üèóÔ∏è Ki·∫øn tr√∫c h·ªá th·ªëng\n",
    "\n",
    "```\n",
    "Webcam Stream ‚Üí MTCNN (Detect) ‚Üí FaceNet (Extract) ‚Üí Cosine Similarity ‚Üí Classification\n",
    "                                                              ‚Üë\n",
    "                                                         Known Database\n",
    "```\n",
    "\n",
    "### **Pipeline x·ª≠ l√Ω:**\n",
    "\n",
    "1Ô∏è‚É£ **Preprocessing** (Offline - 1 l·∫ßn)\n",
    "   - Load ·∫£nh t·ª´ th∆∞ m·ª•c `known/`\n",
    "   - Detect faces ‚Üí Extract embeddings ‚Üí Build database\n",
    "\n",
    "2Ô∏è‚É£ **Real-time Processing** (Online - m·ªói frame)\n",
    "   - Capture frame ‚Üí Detect faces ‚Üí Extract embeddings\n",
    "   - Compare v·ªõi database ‚Üí Compute similarities\n",
    "   - Classify v√† visualize k·∫øt qu·∫£\n",
    "\n",
    "## üî¨ C√°c k·ªπ thu·∫≠t s·ª≠ d·ª•ng\n",
    "\n",
    "### **1. Face Detection - MTCNN**\n",
    "- **T·∫°i sao ch·ªçn MTCNN?**\n",
    "  - Multi-stage cascade: P-Net ‚Üí R-Net ‚Üí O-Net (3 stages)\n",
    "  - Ch√≠nh x√°c cao v·ªõi faces ·ªü nhi·ªÅu g√≥c ƒë·ªô\n",
    "  - Tr·∫£ v·ªÅ bounding box ch√≠nh x√°c v√† confidence score\n",
    "  \n",
    "- **Trong code:**\n",
    "  ```python\n",
    "  detector = MTCNN()\n",
    "  faces = detector.detect_faces(rgb_image)\n",
    "  ```\n",
    "\n",
    "### **2. Face Recognition - FaceNet**\n",
    "- **T·∫°i sao ch·ªçn FaceNet?**\n",
    "  - Bi·ªÉu di·ªÖn khu√¥n m·∫∑t d∆∞·ªõi d·∫°ng vector 512 chi·ªÅu\n",
    "  - Pre-trained tr√™n dataset l·ªõn (VGGFace2, CASIA-WebFace)\n",
    "  - Khu√¥n m·∫∑t gi·ªëng nhau ‚Üí embeddings g·∫ßn nhau trong kh√¥ng gian vector\n",
    "  \n",
    "- **Trong code:**\n",
    "  ```python\n",
    "  embedder = FaceNet()\n",
    "  embedding = embedder.embeddings([face_160x160])[0]  # ‚Üí 512-d vector\n",
    "  ```\n",
    "\n",
    "### **3. Similarity Metric - Cosine Similarity**\n",
    "- **C√¥ng th·ª©c:**\n",
    "  $$\\text{similarity} = \\frac{A \\cdot B}{||A|| \\times ||B||} = \\cos(\\theta)$$\n",
    "  \n",
    "- **Gi·∫£i th√≠ch:**\n",
    "  - ƒêo g√≥c gi·ªØa 2 vectors (kh√¥ng ph·ª• thu·ªôc ƒë·ªô d√†i)\n",
    "  - K·∫øt qu·∫£: [-1, 1]\n",
    "    - 1 = ho√†n to√†n gi·ªëng nhau\n",
    "    - 0 = kh√¥ng li√™n quan\n",
    "    - -1 = ƒë·ªëi l·∫≠p (hi·∫øm x·∫£y ra v·ªõi face embeddings)\n",
    "  \n",
    "- **Trong code:**\n",
    "  ```python\n",
    "  def cosine_similarity(a, b):\n",
    "      a = l2_normalize(a)  # Chu·∫©n h√≥a\n",
    "      b = l2_normalize(b)\n",
    "      return float(np.dot(a, b))\n",
    "  ```\n",
    "\n",
    "### **4. Threshold-based Classification**\n",
    "- **Decision boundary:** similarity = 0.7\n",
    "  ```\n",
    "  if similarity > 0.7:  ‚Üí MATCHED (ng∆∞·ªùi quen)\n",
    "  else:                 ‚Üí UNKNOWN (ng∆∞·ªùi l·∫°)\n",
    "  ```\n",
    "  \n",
    "- **Tuning threshold:**\n",
    "  - TƒÉng (> 0.7): Strict h∆°n ‚Üí √≠t False Positive, nh∆∞ng nhi·ªÅu False Negative\n",
    "  - Gi·∫£m (< 0.7): Loose h∆°n ‚Üí nhi·ªÅu False Positive, √≠t False Negative\n",
    "\n",
    "## ‚öôÔ∏è Chi ti·∫øt tri·ªÉn khai\n",
    "\n",
    "### **A. X·ª≠ l√Ω Database (Offline)**\n",
    "```python\n",
    "for image in known_folder:\n",
    "    faces = detector.detect_faces(image)\n",
    "    face_crop = crop_largest_face(faces)\n",
    "    embedding = embedder.embeddings(face_crop)\n",
    "    known_embeddings.append(embedding)\n",
    "    known_names.append(person_name)\n",
    "```\n",
    "\n",
    "**Optimization:**\n",
    "- Ch·ªâ l∆∞u 1 embedding/ng∆∞·ªùi (l·∫•y face c√≥ confidence cao nh·∫•t)\n",
    "- Convert sang numpy array ƒë·ªÉ vectorize operations\n",
    "\n",
    "### **B. Real-time Recognition (Online)**\n",
    "```python\n",
    "while True:\n",
    "    frame = cap.read()\n",
    "    \n",
    "    # Detect\n",
    "    faces = detector.detect_faces(frame)\n",
    "    \n",
    "    for face in faces:\n",
    "        # Extract\n",
    "        face_crop = preprocess(face)\n",
    "        live_embedding = embedder.embeddings(face_crop)\n",
    "        \n",
    "        # Compare\n",
    "        similarities = [cosine_sim(live_embedding, known_emb) \n",
    "                       for known_emb in known_embeddings]\n",
    "        best_match = max(similarities)\n",
    "        \n",
    "        # Classify\n",
    "        if best_match > THRESHOLD:\n",
    "            label = known_names[argmax(similarities)]\n",
    "        else:\n",
    "            label = \"Unknown\"\n",
    "        \n",
    "        # Visualize\n",
    "        draw_bbox_and_label(frame, face, label)\n",
    "```\n",
    "\n",
    "## üìä ƒê√°nh gi√° hi·ªáu nƒÉng\n",
    "\n",
    "### **T·ªëc ƒë·ªô:**\n",
    "- MTCNN: ~0.1-0.3s/frame (CPU)\n",
    "- FaceNet: ~0.05s/face (CPU)\n",
    "- **Total:** ~10-15 FPS v·ªõi 1 face/frame\n",
    "\n",
    "### **ƒê·ªô ch√≠nh x√°c:**\n",
    "- Ph·ª• thu·ªôc:\n",
    "  - Ch·∫•t l∆∞·ª£ng ·∫£nh trong database\n",
    "  - ƒêi·ªÅu ki·ªán √°nh s√°ng\n",
    "  - G√≥c nh√¨n, bi·ªÉu c·∫£m khu√¥n m·∫∑t\n",
    "- Threshold 0.7: Balance t·ªët gi·ªØa precision v√† recall\n",
    "\n",
    "## üöÄ C·∫£i ti·∫øn c√≥ th·ªÉ th·ª±c hi·ªán\n",
    "\n",
    "1. **Performance:**\n",
    "   - D√πng GPU (TensorFlow-GPU)\n",
    "   - Gi·∫£m resolution input frame\n",
    "   - Skip frames (process m·ªói 2-3 frames)\n",
    "\n",
    "2. **Accuracy:**\n",
    "   - L∆∞u nhi·ªÅu embeddings/ng∆∞·ªùi (nhi·ªÅu g√≥c ƒë·ªô)\n",
    "   - Fine-tune threshold d·ª±a tr√™n validation set\n",
    "   - S·ª≠ d·ª•ng face alignment tr∆∞·ªõc khi extract embeddings\n",
    "\n",
    "3. **Features:**\n",
    "   - L∆∞u log nh·∫≠n di·ªán (timestamp, person)\n",
    "   - Alert khi ph√°t hi·ªán ng∆∞·ªùi l·∫°\n",
    "   - Multi-face tracking v·ªõi unique IDs\n",
    "\n",
    "4. **Robustness:**\n",
    "   - Anti-spoofing (ch·ªëng ·∫£nh in, video)\n",
    "   - Age/expression invariance\n",
    "   - Mask detection\n",
    "\n",
    "## üìö T√†i li·ªáu tham kh·∫£o\n",
    "\n",
    "- **MTCNN Paper:** Zhang et al., \"Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks\" (2016)\n",
    "- **FaceNet Paper:** Schroff et al., \"FaceNet: A Unified Embedding for Face Recognition and Clustering\" (2015)\n",
    "- **keras-facenet:** https://github.com/nyoki-mtl/keras-facenet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
