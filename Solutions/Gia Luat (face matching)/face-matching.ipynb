{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e32b23e",
   "metadata": {},
   "source": [
    "# Nh·∫≠n di·ªán khu√¥n m·∫∑t th·ªùi gian th·ª±c v·ªõi FaceNet & MTCNN tr√™n Webcam\n",
    "\n",
    "B√†i t·∫≠p th·ª±c h√†nh: X√¢y d·ª±ng h·ªá th·ªëng nh·∫≠n di·ªán khu√¥n m·∫∑t theo th·ªùi gian th·ª±c s·ª≠ d·ª•ng:\n",
    "- **MTCNN**: Ph√°t hi·ªán khu√¥n m·∫∑t\n",
    "- **FaceNet**: Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng v√† so s√°nh khu√¥n m·∫∑t\n",
    "- **OpenCV**: Truy c·∫≠p webcam\n",
    "\n",
    "**ƒêi·ªÅu ki·ªán so s√°nh:**\n",
    "- Similarity > 0.7: \"Matched\"\n",
    "- Similarity < 0.7: \"Unknown\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82254472",
   "metadata": {},
   "source": [
    "## 1. Import th∆∞ vi·ªán v√† thi·∫øt l·∫≠p c·∫•u h√¨nh\n",
    "\n",
    "Ph·∫ßn n√†y import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt v√† thi·∫øt l·∫≠p c√°c tham s·ªë quan tr·ªçng:\n",
    "- **MTCNN**: Th∆∞ vi·ªán ph√°t hi·ªán khu√¥n m·∫∑t v·ªõi ƒë·ªô ch√≠nh x√°c cao\n",
    "- **FaceNet**: M√¥ h√¨nh deep learning ƒë·ªÉ tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng khu√¥n m·∫∑t (embeddings 512 chi·ªÅu)\n",
    "- **OpenCV**: X·ª≠ l√Ω ·∫£nh v√† video\n",
    "\n",
    "**C√°c tham s·ªë c·∫•u h√¨nh:**\n",
    "- `KNOWN_DIR`: Th∆∞ m·ª•c ch·ª©a ·∫£nh c·ªßa nh·ªØng ng∆∞·ªùi c·∫ßn nh·∫≠n di·ªán\n",
    "- `THRESHOLD`: Ng∆∞·ª°ng similarity ƒë·ªÉ x√°c ƒë·ªãnh \"Matched\" (0.7 = 70% ƒë·ªô t∆∞∆°ng ƒë·ªìng)\n",
    "- `CAM_INDEX`: Ch·ªâ s·ªë webcam (0 = webcam m·∫∑c ƒë·ªãnh)\n",
    "- `MIN_FACE_SIZE`: K√≠ch th∆∞·ªõc t·ªëi thi·ªÉu c·ªßa khu√¥n m·∫∑t ƒë∆∞·ª£c x·ª≠ l√Ω (pixel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8265fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "from keras_facenet import FaceNet\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "KNOWN_DIR = \"known\"          # th∆∞ m·ª•c ch·ª©a ·∫£nh ng∆∞·ªùi ƒë√£ bi·∫øt\n",
    "THRESHOLD = 0.7             # similarity threshold\n",
    "CAM_INDEX = 0               # webcam index (0 th∆∞·ªùng l√† webcam m·∫∑c ƒë·ªãnh)\n",
    "MIN_FACE_SIZE = 60          # b·ªè qua m·∫∑t qu√° nh·ªè (px)\n",
    "\n",
    "print(\"‚úÖ ƒê√£ import c√°c th∆∞ vi·ªán v√† thi·∫øt l·∫≠p config\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be90375",
   "metadata": {},
   "source": [
    "## 2. ƒê·ªãnh nghƒ©a c√°c h√†m h·ªó tr·ª£ (Helper Functions)\n",
    "\n",
    "C√°c h√†m ti·ªán √≠ch ƒë·ªÉ x·ª≠ l√Ω embeddings v√† ·∫£nh:\n",
    "\n",
    "- **`l2_normalize()`**: Chu·∫©n h√≥a vector theo chu·∫©n L2 (ƒë∆∞a v·ªÅ ƒë·ªô d√†i = 1)\n",
    "- **`cosine_similarity()`**: T√≠nh ƒë·ªô t∆∞∆°ng ƒë·ªìng cosine gi·ªØa 2 embeddings (k·∫øt qu·∫£ t·ª´ -1 ƒë·∫øn 1, c√†ng g·∫ßn 1 = c√†ng gi·ªëng)\n",
    "- **`preprocess_face()`**: Ti·ªÅn x·ª≠ l√Ω khu√¥n m·∫∑t:\n",
    "  - Chuy·ªÉn t·ª´ BGR ‚Üí RGB (OpenCV d√πng BGR, FaceNet c·∫ßn RGB)\n",
    "  - Resize v·ªÅ 160√ó160 pixels (k√≠ch th∆∞·ªõc input c·ªßa FaceNet)\n",
    "- **`get_embedding()`**: Tr√≠ch xu·∫•t vector ƒë·∫∑c tr∆∞ng 512 chi·ªÅu t·ª´ ·∫£nh khu√¥n m·∫∑t\n",
    "- **`safe_crop()`**: Crop ·∫£nh an to√†n kh√¥ng b·ªã v∆∞·ª£t bi√™n frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d04a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "def l2_normalize(x: np.ndarray, eps: float = 1e-10) -> np.ndarray:\n",
    "    \"\"\"Chu·∫©n h√≥a vector theo L2\"\"\"\n",
    "    return x / (np.linalg.norm(x) + eps)\n",
    "\n",
    "def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    \"\"\"T√≠nh ƒë·ªô t∆∞∆°ng ƒë·ªìng cosine gi·ªØa 2 vectors\"\"\"\n",
    "    a = l2_normalize(a)\n",
    "    b = l2_normalize(b)\n",
    "    return float(np.dot(a, b))\n",
    "\n",
    "def preprocess_face(face_bgr: np.ndarray, target_size=(160, 160)) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    FaceNet th∆∞·ªùng d√πng input 160x160, RGB.\n",
    "    Tr·∫£ v·ªÅ m·∫£ng (160,160,3) RGB uint8.\n",
    "    \"\"\"\n",
    "    face_rgb = cv2.cvtColor(face_bgr, cv2.COLOR_BGR2RGB)\n",
    "    face_rgb = cv2.resize(face_rgb, target_size, interpolation=cv2.INTER_AREA)\n",
    "    return face_rgb\n",
    "\n",
    "def get_embedding(embedder: FaceNet, face_rgb_160: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    keras-facenet: embeddings() nh·∫≠n list/np array ·∫£nh RGB (uint8 ok).\n",
    "    \"\"\"\n",
    "    emb = embedder.embeddings([face_rgb_160])[0]  # shape (512,)\n",
    "    return emb.astype(np.float32)\n",
    "\n",
    "def safe_crop(frame: np.ndarray, x: int, y: int, w: int, h: int) -> tuple:\n",
    "    \"\"\"Crop an to√†n, ƒë·∫£m b·∫£o kh√¥ng v∆∞·ª£t bi√™n\"\"\"\n",
    "    H, W = frame.shape[:2]\n",
    "    x1 = max(0, x)\n",
    "    y1 = max(0, y)\n",
    "    x2 = min(W, x + w)\n",
    "    y2 = min(H, y + h)\n",
    "    return frame[y1:y2, x1:x2], (x1, y1, x2, y2)\n",
    "\n",
    "print(\"‚úÖ ƒê√£ ƒë·ªãnh nghƒ©a c√°c helper functions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88b5fee",
   "metadata": {},
   "source": [
    "## 3. Kh·ªüi t·∫°o c√°c m√¥ h√¨nh AI\n",
    "\n",
    "Load 2 m√¥ h√¨nh deep learning ch√≠nh:\n",
    "\n",
    "1. **MTCNN (Multi-task Cascaded Convolutional Networks)**:\n",
    "   - Ph√°t hi·ªán khu√¥n m·∫∑t trong ·∫£nh/video\n",
    "   - Tr·∫£ v·ªÅ bounding box v√† confidence score\n",
    "   - Ho·∫°t ƒë·ªông t·ªët v·ªõi nhi·ªÅu khu√¥n m·∫∑t, g√≥c nghi√™ng, v√† ƒëi·ªÅu ki·ªán √°nh s√°ng kh√°c nhau\n",
    "\n",
    "2. **FaceNet (keras-facenet)**:\n",
    "   - M√¥ h√¨nh CNN pre-trained tr√™n h√†ng tri·ªáu khu√¥n m·∫∑t\n",
    "   - Chuy·ªÉn ƒë·ªïi ·∫£nh khu√¥n m·∫∑t th√†nh vector 512 chi·ªÅu\n",
    "   - C√°c khu√¥n m·∫∑t gi·ªëng nhau c√≥ embeddings g·∫ßn nhau trong kh√¥ng gian vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28436c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# LOAD MODELS\n",
    "# =========================\n",
    "print(\"ƒêang load models...\")\n",
    "detector = MTCNN()\n",
    "embedder = FaceNet()  # load FaceNet\n",
    "print(\"‚úÖ MTCNN v√† FaceNet ƒë√£ s·∫µn s√†ng!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e7eb51",
   "metadata": {},
   "source": [
    "## 4. X√¢y d·ª±ng c∆° s·ªü d·ªØ li·ªáu khu√¥n m·∫∑t (Database)\n",
    "\n",
    "T·∫°o database embeddings t·ª´ ·∫£nh trong th∆∞ m·ª•c `known/`:\n",
    "\n",
    "**Quy tr√¨nh:**\n",
    "1. T·∫°o th∆∞ m·ª•c `known/` n·∫øu ch∆∞a t·ªìn t·∫°i\n",
    "2. Duy·ªát qua t·∫•t c·∫£ file ·∫£nh (.jpg, .jpeg, .png)\n",
    "3. V·ªõi m·ªói ·∫£nh:\n",
    "   - ƒê·ªçc ·∫£nh v√† ph√°t hi·ªán khu√¥n m·∫∑t b·∫±ng MTCNN\n",
    "   - L·∫•y khu√¥n m·∫∑t c√≥ confidence cao nh·∫•t (n·∫øu c√≥ nhi·ªÅu ng∆∞·ªùi)\n",
    "   - Crop v√† preprocess khu√¥n m·∫∑t\n",
    "   - Tr√≠ch xu·∫•t embedding 512 chi·ªÅu b·∫±ng FaceNet\n",
    "   - L∆∞u embedding v√† t√™n ng∆∞·ªùi (t√™n file kh√¥ng ƒëu√¥i) v√†o arrays\n",
    "4. Chuy·ªÉn sang numpy array ƒë·ªÉ t√≠nh to√°n hi·ªáu qu·∫£\n",
    "\n",
    "**K·∫øt qu·∫£:** Danh s√°ch `known_embeddings` v√† `known_names` ƒë·ªÉ so s√°nh real-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bd5189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# BUILD KNOWN DATABASE\n",
    "# =========================\n",
    "known_embeddings = []\n",
    "known_names = []\n",
    "\n",
    "# T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥\n",
    "if not os.path.isdir(KNOWN_DIR):\n",
    "    os.makedirs(KNOWN_DIR, exist_ok=True)\n",
    "    print(f\"[!] ƒê√£ t·∫°o th∆∞ m·ª•c '{KNOWN_DIR}'. H√£y b·ªè ·∫£nh ng∆∞·ªùi quen v√†o ƒë√≥ r·ªìi ch·∫°y l·∫°i.\")\n",
    "else:\n",
    "    image_files = [f for f in os.listdir(KNOWN_DIR) if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "    \n",
    "    if len(image_files) == 0:\n",
    "        print(f\"[!] Th∆∞ m·ª•c '{KNOWN_DIR}' ch∆∞a c√≥ ·∫£nh. H√£y th√™m ·∫£nh (jpg/png) r·ªìi ch·∫°y l·∫°i.\")\n",
    "    else:\n",
    "        print(f\"[*] ƒêang t·∫°o database embeddings t·ª´ th∆∞ m·ª•c {KNOWN_DIR}/ ...\")\n",
    "        \n",
    "        for fn in image_files:\n",
    "            path = os.path.join(KNOWN_DIR, fn)\n",
    "            img = cv2.imread(path)\n",
    "            if img is None:\n",
    "                print(f\"[!] Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c: {path}\")\n",
    "                continue\n",
    "\n",
    "            # detect face in known image\n",
    "            faces = detector.detect_faces(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "            if len(faces) == 0:\n",
    "                print(f\"[!] Kh√¥ng ph√°t hi·ªán m·∫∑t trong: {fn}\")\n",
    "                continue\n",
    "\n",
    "            # l·∫•y face c√≥ confidence cao nh·∫•t\n",
    "            faces = sorted(faces, key=lambda d: d.get(\"confidence\", 0), reverse=True)\n",
    "            x, y, w, h = faces[0][\"box\"]\n",
    "            face_crop, _ = safe_crop(img, x, y, w, h)\n",
    "\n",
    "            if face_crop.size == 0 or min(face_crop.shape[:2]) < MIN_FACE_SIZE:\n",
    "                print(f\"[!] M·∫∑t qu√° nh·ªè/l·ªói crop trong: {fn}\")\n",
    "                continue\n",
    "\n",
    "            face_160 = preprocess_face(face_crop)\n",
    "            emb = get_embedding(embedder, face_160)\n",
    "\n",
    "            name = os.path.splitext(fn)[0]  # t√™n = filename kh√¥ng ƒëu√¥i\n",
    "            known_embeddings.append(emb)\n",
    "            known_names.append(name)\n",
    "            print(f\"    + Loaded: {name}\")\n",
    "\n",
    "        known_embeddings = np.array(known_embeddings, dtype=np.float32)\n",
    "        \n",
    "        if len(known_embeddings) == 0:\n",
    "            print(\"[!] Kh√¥ng t·∫°o ƒë∆∞·ª£c embeddings n√†o t·ª´ known/. H√£y d√πng ·∫£nh r√µ m·∫∑t h∆°n.\")\n",
    "        else:\n",
    "            print(f\"\\n‚úÖ Database s·∫µn s√†ng: {len(known_embeddings)} ng∆∞·ªùi\")\n",
    "            print(f\"üìã Danh s√°ch: {', '.join(known_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dbdd77",
   "metadata": {},
   "source": [
    "## 5. Nh·∫≠n di·ªán khu√¥n m·∫∑t theo th·ªùi gian th·ª±c (Real-time Recognition)\n",
    "\n",
    "X·ª≠ l√Ω video t·ª´ webcam v√† nh·∫≠n di·ªán khu√¥n m·∫∑t frame-by-frame:\n",
    "\n",
    "**Lu·ªìng x·ª≠ l√Ω m·ªói frame:**\n",
    "1. Capture frame t·ª´ webcam v√† flip ngang (hi·ªáu ·ª©ng g∆∞∆°ng)\n",
    "2. Chuy·ªÉn sang RGB v√† ph√°t hi·ªán khu√¥n m·∫∑t b·∫±ng MTCNN\n",
    "3. V·ªõi m·ªói khu√¥n m·∫∑t ƒë∆∞·ª£c ph√°t hi·ªán:\n",
    "   - Ki·ªÉm tra confidence > 0.90 (l·ªçc detection k√©m)\n",
    "   - Crop v√† preprocess khu√¥n m·∫∑t\n",
    "   - Tr√≠ch xu·∫•t embedding\n",
    "   - **So s√°nh v·ªõi database**: T√≠nh cosine similarity v·ªõi t·∫•t c·∫£ known embeddings\n",
    "   - L·∫•y similarity cao nh·∫•t v√† ki·ªÉm tra threshold:\n",
    "     - **> 0.7**: Matched ‚Üí v·∫Ω box XANH L√Å + hi·ªÉn th·ªã t√™n\n",
    "     - **‚â§ 0.7**: Unknown ‚Üí v·∫Ω box ƒê·ªé\n",
    "4. Hi·ªÉn th·ªã frame v·ªõi bounding boxes, t√™n, v√† similarity scores\n",
    "5. Nh·∫•n 'q' ƒë·ªÉ tho√°t\n",
    "\n",
    "**T·ªëi ∆∞u:**\n",
    "- B·ªè qua khu√¥n m·∫∑t qu√° nh·ªè (< 60px)\n",
    "- Hi·ªÉn th·ªã confidence c·ªßa MTCNN\n",
    "- ƒê·∫øm s·ªë frames ƒë√£ x·ª≠ l√Ω\n",
    "\n",
    "**Vi·ªác ch·ªâ x·ª≠ l√Ω c√°c khu√¥n m·∫∑t c√≥ confidence > 0.90 nh·∫±m:**\n",
    "\n",
    "Lo·∫°i b·ªè c√°c ph√°t hi·ªán k√©m ch√≠nh x√°c do nhi·ªÖu n·ªÅn, √°nh s√°ng y·∫øu ho·∫∑c v·∫≠t th·ªÉ gi·ªëng khu√¥n m·∫∑t.\n",
    "\n",
    "Gi·∫£m sai s·ªë khi tr√≠ch xu·∫•t embedding: n·∫øu v√πng crop kh√¥ng ph·∫£i khu√¥n m·∫∑t th·∫≠t, FaceNet s·∫Ω sinh vector ƒë·∫∑c tr∆∞ng sai l·ªách, d·∫´n ƒë·∫øn nh·∫≠n di·ªán nh·∫ßm.\n",
    "\n",
    "TƒÉng ƒë·ªô ·ªïn ƒë·ªãnh c·ªßa h·ªá th·ªëng realtime, tr√°nh vi·ªác nh√£n ‚ÄúMatched/Unknown‚Äù nh·∫•p nh√°y li√™n t·ª•c.\n",
    "\n",
    "T·ªëi ∆∞u hi·ªáu nƒÉng, ch·ªâ x·ª≠ l√Ω nh·ªØng khu√¥n m·∫∑t c√≥ ch·∫•t l∆∞·ª£ng t·ªët, gi·∫£m t·∫£i t√≠nh to√°n kh√¥ng c·∫ßn thi·∫øt.\n",
    "\n",
    "Ng∆∞·ª°ng 0.90 ƒë∆∞·ª£c ch·ªçn d·ª±a tr√™n th·ª±c nghi·ªám, ƒë·∫£m b·∫£o c√¢n b·∫±ng gi·ªØa ƒë·ªô ch√≠nh x√°c ph√°t hi·ªán v√† kh·∫£ nƒÉng nh·∫≠n di·ªán ·ªïn ƒë·ªãnh trong m√¥i tr∆∞·ªùng webcam th·ªùi gian th·ª±c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af5013d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# REALTIME WEBCAM\n",
    "# =========================\n",
    "if len(known_embeddings) == 0:\n",
    "    print(\"‚ö†Ô∏è  Kh√¥ng c√≥ database ƒë·ªÉ so s√°nh. H√£y th√™m ·∫£nh v√†o th∆∞ m·ª•c 'known/' tr∆∞·ªõc!\")\n",
    "else:\n",
    "    cap = cv2.VideoCapture(CAM_INDEX)\n",
    "    if not cap.isOpened():\n",
    "        print(\"[!] Kh√¥ng m·ªü ƒë∆∞·ª£c webcam.\")\n",
    "    else:\n",
    "        print(\"[*] Webcam ƒë√£ s·∫µn s√†ng. Nh·∫•n 'q' ƒë·ªÉ tho√°t.\")\n",
    "        print(f\"[*] Threshold: {THRESHOLD}\")\n",
    "        print(f\"[*] S·ªë ng∆∞·ªùi trong database: {len(known_names)}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        frame_count = 0\n",
    "        \n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Flip frame ƒë·ªÉ t·∫°o hi·ªáu ·ª©ng g∆∞∆°ng\n",
    "            frame = cv2.flip(frame, 1)\n",
    "            \n",
    "            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            detections = detector.detect_faces(rgb)\n",
    "\n",
    "            for det in detections:\n",
    "                conf = det.get(\"confidence\", 0)\n",
    "                if conf < 0.90:  # b·ªè qua detection c√≥ confidence th·∫•p\n",
    "                    continue\n",
    "\n",
    "                x, y, w, h = det[\"box\"]\n",
    "                face_crop, (x1, y1, x2, y2) = safe_crop(frame, x, y, w, h)\n",
    "\n",
    "                if face_crop.size == 0:\n",
    "                    continue\n",
    "                if min(face_crop.shape[:2]) < MIN_FACE_SIZE:\n",
    "                    continue\n",
    "\n",
    "                face_160 = preprocess_face(face_crop)\n",
    "                emb_live = get_embedding(embedder, face_160)\n",
    "\n",
    "                # So kh·ªõp: l·∫•y similarity l·ªõn nh·∫•t\n",
    "                sims = [cosine_similarity(emb_live, e) for e in known_embeddings]\n",
    "                best_idx = int(np.argmax(sims))\n",
    "                best_sim = float(sims[best_idx])\n",
    "                best_name = known_names[best_idx]\n",
    "\n",
    "                # Ki·ªÉm tra threshold\n",
    "                if best_sim > THRESHOLD:\n",
    "                    label = \"Matched\"\n",
    "                    color = (0, 255, 0)  # XANH L√Å\n",
    "                    # display_text = f\"{best_name} | sim={best_sim:.2f}\"  #hi·ªÉn th·ªã t√™n\n",
    "                    display_text = f\"MATCHED | sim={best_sim:.2f}\"\n",
    "                else:\n",
    "                    label = \"Unknown\"\n",
    "                    color = (0, 0, 255)  # ƒê·ªé\n",
    "                    display_text = f\"UNKNOWN | sim={best_sim:.2f}\"\n",
    "\n",
    "                # V·∫Ω bounding box\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "                \n",
    "                # V·∫Ω background cho text\n",
    "                (tw, th), _ = cv2.getTextSize(display_text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n",
    "                cv2.rectangle(frame, (x1, y1-30), (x1+tw, y1), color, -1)\n",
    "                \n",
    "                # V·∫Ω text\n",
    "                cv2.putText(frame, display_text, (x1, y1-10), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "                \n",
    "                # V·∫Ω confidence\n",
    "                cv2.putText(frame, f\"Conf: {conf:.2f}\", (x1, y2+20),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "            # Hi·ªÉn th·ªã h∆∞·ªõng d·∫´n\n",
    "            cv2.putText(frame, \"Press 'q' to quit\", (10, 30),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "            \n",
    "            # Hi·ªÉn th·ªã frame\n",
    "            cv2.imshow(\"FaceNet + MTCNN Realtime\", frame)\n",
    "            \n",
    "            frame_count += 1\n",
    "            \n",
    "            if cv2.waitKey(25) & 0xFF == ord(\"q\"):\n",
    "                break\n",
    "\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        print(\"\\n‚úÖ ƒê√£ ƒë√≥ng webcam v√† gi·∫£i ph√≥ng t√†i nguy√™n\")\n",
    "        print(f\"üìä T·ªïng s·ªë frames x·ª≠ l√Ω: {frame_count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
